{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd \n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create db connection to load data\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"movies\", \n",
    "    user=\"docker\",\n",
    "    password=\"docker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into dataframe \n",
    "sql = \"\"\"SELECT * FROM movie\"\"\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql)\n",
    "movies = cursor.fetchall()\n",
    "cursor.close()\n",
    "\n",
    "columns = ['id', 'title', 'release_year', 'release_month', 'imdb_id', 'tmdb_id', 'runtime', 'rating', 'budget', 'imdb_rating', 'rt_rating', 'genre_id']\n",
    "movie_df = pd.DataFrame(movies, columns =columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load actor data\n",
    "sql = \"\"\"\n",
    "    SELECT a.id, a.actor_performance, a.tmdb_id, a.actor_name, ma.movie_id\n",
    "        from actor a\n",
    "    JOIN movie_actor ma \n",
    "    \tON a.id = ma.actor_id\n",
    "\"\"\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(sql)\n",
    "actors = cursor.fetchall()\n",
    "cursor.close()\n",
    "\n",
    "columns = ['actor_id', 'actor_performance', 'tmdb_id', 'actor_name', 'movie_id']\n",
    "actor_df = pd.DataFrame(actors, columns=columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add a column that averages a movies actors performance ratings together. \n",
    "# I know it's not recommended to iterate through a dataframe but it was tricky with \n",
    "# the way I set up my data to do another way \n",
    "movie_df['actor_performance'] = ''\n",
    "for index, row in movie_df.iterrows():\n",
    "    movie_id = row['id']\n",
    "    movie_actors = actor_df.loc[actor_df['movie_id'] == movie_id]\n",
    "    actor_average_performance = movie_actors['actor_performance'].mean()\n",
    "    row['actor_performance'] = actor_average_performance\n",
    "    movie_df.at[index,'actor_performance'] = actor_average_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've got data loaded and now want to clean some of the columns. Year is \n",
    "# irrelevant to our prediction model so I'll drop that column. \n",
    "del movie_df['release_year']\n",
    "# title, id, imdb_id tmdb_id will be irrelevant to the model  \n",
    "del movie_df['title']\n",
    "del movie_df['imdb_id']\n",
    "del movie_df['tmdb_id']\n",
    "del movie_df['id']\n",
    "# rotten tomatoes rating is irrelevant. I collected it in case I wanted \n",
    "# to also predict that rating but will remove it for sake of time\n",
    "del movie_df['rt_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2409\n6\n6\n"
    }
   ],
   "source": [
    "# check column values to see if enough data is present to use them in model\n",
    "# the following line shows that the budget value is missing from about half of the \n",
    "# rows. This is unfortunate because I'm sure that has an influence on the success\n",
    "# of a movie. Also check to make sure all movies have a imdb rating\n",
    "print((movie_df['budget'] == 0).sum())\n",
    "print(pd.isna(movie_df['imdb_rating']).sum())\n",
    "print(pd.isna(movie_df['actor_performance']).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del movie_df['budget']\n",
    "movie_df = movie_df[movie_df['imdb_rating'].notna()]\n",
    "movie_df = movie_df[movie_df['actor_performance'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "release_month           Int64\nruntime                 Int64\nrating               category\nimdb_rating           float64\ngenre_id                Int64\nactor_performance     float64\ndtype: object"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# clean up remaining columns before building model\n",
    "movie_df.rating = pd.Categorical(movie_df.rating)\n",
    "movie_df = movie_df.convert_dtypes()\n",
    "movie_df['imdb_rating'] = pd.to_numeric(movie_df['imdb_rating'])\n",
    "movie_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   release_month   runtime  imdb_rating  genre_id  \\\nrelease_month           1.000000  0.075686     0.060432  0.017597   \nruntime                 0.075686  1.000000     0.436286  0.010636   \nimdb_rating             0.060432  0.436286     1.000000  0.013098   \ngenre_id                0.017597  0.010636     0.013098  1.000000   \nactor_performance       0.047477  0.372030     0.664407  0.064042   \n\n                   actor_performance  \nrelease_month               0.047477  \nruntime                     0.372030  \nimdb_rating                 0.664407  \ngenre_id                    0.064042  \nactor_performance           1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>release_month</th>\n      <th>runtime</th>\n      <th>imdb_rating</th>\n      <th>genre_id</th>\n      <th>actor_performance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>release_month</th>\n      <td>1.000000</td>\n      <td>0.075686</td>\n      <td>0.060432</td>\n      <td>0.017597</td>\n      <td>0.047477</td>\n    </tr>\n    <tr>\n      <th>runtime</th>\n      <td>0.075686</td>\n      <td>1.000000</td>\n      <td>0.436286</td>\n      <td>0.010636</td>\n      <td>0.372030</td>\n    </tr>\n    <tr>\n      <th>imdb_rating</th>\n      <td>0.060432</td>\n      <td>0.436286</td>\n      <td>1.000000</td>\n      <td>0.013098</td>\n      <td>0.664407</td>\n    </tr>\n    <tr>\n      <th>genre_id</th>\n      <td>0.017597</td>\n      <td>0.010636</td>\n      <td>0.013098</td>\n      <td>1.000000</td>\n      <td>0.064042</td>\n    </tr>\n    <tr>\n      <th>actor_performance</th>\n      <td>0.047477</td>\n      <td>0.372030</td>\n      <td>0.664407</td>\n      <td>0.064042</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# check correlations\n",
    "movie_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our variable correlation to imdb_rating does not look very good. Actor performance is at .6644 which is very low in my opinion. Going to continue on with building the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "x = movie_df[['release_month', 'runtime', 'imdb_rating', 'genre_id', 'actor_performance']]\n",
    "y = movie_df['imdb_rating']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 1.16787642e-16 -3.85108612e-16  1.00000000e+00 -8.67361738e-17\n -2.06892880e-16]\n"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9.840187567054174e-15\n9.840187567054174e-15\n1.2750214361728412e-14\n"
    }
   ],
   "source": [
    "# predictions and performance\n",
    "predictions = model.predict(x_test)\n",
    "print(metrics.mean_absolute_error(y_test, predictions))\n",
    "print(metrics.mean_absolute_error(y_test, predictions))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RMS value is very low which indicates that our model might not fit the data well. \n",
    "\n",
    "Calculating p values of our coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            imdb_rating   R-squared:                       0.484\nModel:                            OLS   Adj. R-squared:                  0.484\nMethod:                 Least Squares   F-statistic:                     1166.\nDate:                Sun, 11 Oct 2020   Prob (F-statistic):               0.00\nTime:                        21:28:04   Log-Likelihood:                -5701.2\nNo. Observations:                4977   AIC:                         1.141e+04\nDf Residuals:                    4972   BIC:                         1.145e+04\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.5133      0.131    -19.149      0.000      -2.771      -2.256\nx1             0.0053      0.003      1.631      0.103      -0.001       0.012\nx2             0.0120      0.001     19.816      0.000       0.011       0.013\nx3            -0.0002    8.8e-05     -2.637      0.008      -0.000   -5.96e-05\nx4             1.1487      0.022     53.109      0.000       1.106       1.191\n==============================================================================\nOmnibus:                      285.219   Durbin-Watson:                   1.975\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              455.662\nSkew:                          -0.472   Prob(JB):                     1.13e-99\nKurtosis:                       4.143   Cond. No.                     2.12e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.12e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>       <td>imdb_rating</td>   <th>  R-squared:         </th> <td>   0.484</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.484</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1166.</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 11 Oct 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>21:28:04</td>     <th>  Log-Likelihood:    </th> <td> -5701.2</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  4977</td>      <th>  AIC:               </th> <td>1.141e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  4972</td>      <th>  BIC:               </th> <td>1.145e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>   -2.5133</td> <td>    0.131</td> <td>  -19.149</td> <td> 0.000</td> <td>   -2.771</td> <td>   -2.256</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.0053</td> <td>    0.003</td> <td>    1.631</td> <td> 0.103</td> <td>   -0.001</td> <td>    0.012</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.0120</td> <td>    0.001</td> <td>   19.816</td> <td> 0.000</td> <td>    0.011</td> <td>    0.013</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>   -0.0002</td> <td>  8.8e-05</td> <td>   -2.637</td> <td> 0.008</td> <td>   -0.000</td> <td>-5.96e-05</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>    1.1487</td> <td>    0.022</td> <td>   53.109</td> <td> 0.000</td> <td>    1.106</td> <td>    1.191</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>285.219</td> <th>  Durbin-Watson:     </th> <td>   1.975</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 455.662</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.472</td>  <th>  Prob(JB):          </th> <td>1.13e-99</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 4.143</td>  <th>  Cond. No.          </th> <td>2.12e+03</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.12e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x2 = np.column_stack((movie_df['release_month'], movie_df['runtime'], movie_df['genre_id'], movie_df['actor_performance']))\n",
    "y2 =  y[0:]\n",
    "\n",
    "x3 = sm.add_constant(x2)\n",
    "est = sm.OLS(y2, x3.astype(float))\n",
    "est2 = est.fit()\n",
    "est2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p values of our coefficient our all within an acceptable range (<.05) except for release_month. \n",
    "\n",
    "Overall our linear model does not seem to be a good fit. It seems there is a lot of variance between our predicted values and training values of our data. Let's see if a neural network improves performance at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform x variables to be between 0 and 1\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scale = min_max_scaler.fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scale, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the \"architecture\" of our neural model to be 3 layers with 32 nodes in each layer\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(32, activation = 'relu', input_dim = 5))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model.add(Dense(units = 32, activation = 'relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "model.add(Dense(units = 32, activation = 'relu'))\n",
    "model.add(Dense(units = 1, activation = 'relu'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# hist = neural_model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\n399/399 [==============================] - 0s 1ms/step - loss: 3.9029\nEpoch 2/100\n399/399 [==============================] - 0s 1ms/step - loss: 0.0280\nEpoch 3/100\n399/399 [==============================] - 0s 1ms/step - loss: 0.0028\nEpoch 4/100\n399/399 [==============================] - 0s 1ms/step - loss: 3.8553e-04\nEpoch 5/100\n399/399 [==============================] - 1s 1ms/step - loss: 2.5084e-04\nEpoch 6/100\n399/399 [==============================] - 0s 1ms/step - loss: 2.1427e-04\nEpoch 7/100\n399/399 [==============================] - 0s 1ms/step - loss: 2.0112e-04\nEpoch 8/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.3664e-04\nEpoch 9/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.2160e-04\nEpoch 10/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.0329e-04\nEpoch 11/100\n399/399 [==============================] - 0s 1ms/step - loss: 9.1638e-05\nEpoch 12/100\n399/399 [==============================] - 0s 1ms/step - loss: 8.1945e-05\nEpoch 13/100\n399/399 [==============================] - 0s 1ms/step - loss: 7.1250e-05\nEpoch 14/100\n399/399 [==============================] - 1s 1ms/step - loss: 9.3081e-05\nEpoch 15/100\n399/399 [==============================] - 0s 1ms/step - loss: 9.4155e-05\nEpoch 16/100\n399/399 [==============================] - 0s 1ms/step - loss: 9.7649e-05\nEpoch 17/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.2428e-04\nEpoch 18/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.3781e-04\nEpoch 19/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.2183e-04\nEpoch 20/100\n399/399 [==============================] - 1s 1ms/step - loss: 9.6418e-05\nEpoch 21/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.3180e-04\nEpoch 22/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.0505e-04\nEpoch 23/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.9404e-04\nEpoch 24/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.5547e-04\nEpoch 25/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.1663e-04\nEpoch 26/100\n399/399 [==============================] - 0s 1ms/step - loss: 9.5404e-05\nEpoch 27/100\n399/399 [==============================] - 0s 1ms/step - loss: 5.5325e-05\nEpoch 28/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.5018e-04\nEpoch 29/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.2315e-04\nEpoch 30/100\n399/399 [==============================] - 0s 1ms/step - loss: 4.6455e-05\nEpoch 31/100\n399/399 [==============================] - 1s 1ms/step - loss: 9.1814e-05\nEpoch 32/100\n399/399 [==============================] - 1s 1ms/step - loss: 9.9552e-05\nEpoch 33/100\n399/399 [==============================] - 1s 1ms/step - loss: 6.8083e-05\nEpoch 34/100\n399/399 [==============================] - 1s 2ms/step - loss: 2.0754e-04\nEpoch 35/100\n399/399 [==============================] - 0s 1ms/step - loss: 9.5350e-05\nEpoch 36/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.3209e-04\nEpoch 37/100\n399/399 [==============================] - 1s 1ms/step - loss: 3.3409e-05\nEpoch 38/100\n399/399 [==============================] - 1s 2ms/step - loss: 3.0225e-04\nEpoch 39/100\n399/399 [==============================] - 1s 2ms/step - loss: 3.9892e-05\nEpoch 40/100\n399/399 [==============================] - 1s 2ms/step - loss: 2.1898e-05\nEpoch 41/100\n399/399 [==============================] - 1s 2ms/step - loss: 1.3538e-04\nEpoch 42/100\n399/399 [==============================] - 1s 2ms/step - loss: 8.7216e-05\nEpoch 43/100\n399/399 [==============================] - 1s 1ms/step - loss: 8.3615e-05\nEpoch 44/100\n399/399 [==============================] - 1s 2ms/step - loss: 1.9986e-04\nEpoch 45/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.0996e-04\nEpoch 46/100\n399/399 [==============================] - 1s 1ms/step - loss: 2.7621e-05\nEpoch 47/100\n399/399 [==============================] - 1s 1ms/step - loss: 7.7105e-05\nEpoch 48/100\n399/399 [==============================] - 0s 1ms/step - loss: 8.9420e-05\nEpoch 49/100\n399/399 [==============================] - 1s 1ms/step - loss: 7.3636e-05\nEpoch 50/100\n399/399 [==============================] - 1s 2ms/step - loss: 7.1172e-05\nEpoch 51/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.0142e-04\nEpoch 52/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.0691e-04\nEpoch 53/100\n399/399 [==============================] - 0s 1ms/step - loss: 5.1513e-05\nEpoch 54/100\n399/399 [==============================] - 1s 1ms/step - loss: 2.1772e-04\nEpoch 55/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.9290e-05\nEpoch 56/100\n399/399 [==============================] - 1s 1ms/step - loss: 6.5235e-05\nEpoch 57/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.3671e-04\nEpoch 58/100\n399/399 [==============================] - 0s 1ms/step - loss: 6.2025e-05\nEpoch 59/100\n399/399 [==============================] - 0s 1ms/step - loss: 3.0217e-04\nEpoch 60/100\n399/399 [==============================] - 0s 1ms/step - loss: 7.6283e-06\nEpoch 61/100\n399/399 [==============================] - 1s 2ms/step - loss: 2.3073e-05\nEpoch 62/100\n399/399 [==============================] - 1s 2ms/step - loss: 4.7054e-05\nEpoch 63/100\n399/399 [==============================] - 1s 1ms/step - loss: 2.5205e-04\nEpoch 64/100\n399/399 [==============================] - 1s 1ms/step - loss: 1.6875e-05\nEpoch 65/100\n399/399 [==============================] - 1s 2ms/step - loss: 9.5578e-05\nEpoch 66/100\n399/399 [==============================] - 1s 2ms/step - loss: 6.6446e-05\nEpoch 67/100\n399/399 [==============================] - 1s 2ms/step - loss: 1.8010e-04\nEpoch 68/100\n399/399 [==============================] - 1s 2ms/step - loss: 1.5872e-05\nEpoch 69/100\n399/399 [==============================] - 1s 2ms/step - loss: 2.5426e-05\nEpoch 70/100\n399/399 [==============================] - 1s 1ms/step - loss: 8.7083e-05\nEpoch 71/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.3169e-04\nEpoch 72/100\n399/399 [==============================] - 0s 1ms/step - loss: 2.2458e-05\nEpoch 73/100\n399/399 [==============================] - 0s 1ms/step - loss: 6.8019e-05\nEpoch 74/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.1015e-04\nEpoch 75/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.0394e-04\nEpoch 76/100\n399/399 [==============================] - 0s 1ms/step - loss: 4.4194e-05\nEpoch 77/100\n399/399 [==============================] - 1s 1ms/step - loss: 4.7393e-05\nEpoch 78/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.2244e-04\nEpoch 79/100\n399/399 [==============================] - 0s 1ms/step - loss: 6.8673e-05\nEpoch 80/100\n399/399 [==============================] - 0s 988us/step - loss: 6.7561e-05\nEpoch 81/100\n399/399 [==============================] - 0s 1ms/step - loss: 8.8593e-05\nEpoch 82/100\n399/399 [==============================] - 0s 1ms/step - loss: 4.3664e-05\nEpoch 83/100\n399/399 [==============================] - 0s 1ms/step - loss: 2.6826e-04\nEpoch 84/100\n399/399 [==============================] - 0s 1ms/step - loss: 5.7185e-05\nEpoch 85/100\n399/399 [==============================] - 0s 1ms/step - loss: 6.3797e-06\nEpoch 86/100\n399/399 [==============================] - 0s 1ms/step - loss: 5.9668e-05\nEpoch 87/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.8839e-04\nEpoch 88/100\n399/399 [==============================] - 0s 1ms/step - loss: 7.5491e-06\nEpoch 89/100\n399/399 [==============================] - 0s 1ms/step - loss: 2.4715e-05\nEpoch 90/100\n399/399 [==============================] - 0s 1ms/step - loss: 8.5721e-05\nEpoch 91/100\n399/399 [==============================] - 0s 1ms/step - loss: 7.0448e-05\nEpoch 92/100\n399/399 [==============================] - 0s 1ms/step - loss: 2.3482e-04\nEpoch 93/100\n399/399 [==============================] - 0s 1ms/step - loss: 4.8073e-06\nEpoch 94/100\n399/399 [==============================] - 0s 1ms/step - loss: 5.9433e-05\nEpoch 95/100\n399/399 [==============================] - 0s 1ms/step - loss: 4.8249e-05\nEpoch 96/100\n399/399 [==============================] - 0s 1ms/step - loss: 2.4241e-04\nEpoch 97/100\n399/399 [==============================] - 0s 980us/step - loss: 7.8286e-06\nEpoch 98/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.5062e-04\nEpoch 99/100\n399/399 [==============================] - 0s 1ms/step - loss: 1.0280e-04\nEpoch 100/100\n399/399 [==============================] - 0s 1ms/step - loss: 5.7221e-06\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2216cc45e50>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "derp\nBaseline: -0.02 (0.03) MSE\n"
    }
   ],
   "source": [
    "# print('derp')\n",
    "# estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
    "# kfold = KFold(n_splits=10)\n",
    "# results = cross_val_score(estimator, x_scale, y, cv=kfold)\n",
    "# print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bit3c9d784710324e61add287772fbc9e44",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}